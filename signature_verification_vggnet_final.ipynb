{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": " signature verification vggnet final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakmoud/Signature-Verification/blob/master/signature_verification_vggnet_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VX9U2sdL_6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "faede79d-27aa-4f56-9463-cd40284e4a81"
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "img_width, img_height = 128, 128\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7EgTgOwL_68",
        "colab_type": "code",
        "outputId": "4019eed3-642e-434d-b698-d3180ef12e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Load the pretrained Network\n",
        "vgg16_model = VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(img_height,img_width,3), pooling=None, classes=1000)\n",
        "print(\"pretrained Network is loaded\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "pretrained Network is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI6stXzxVEW3",
        "colab_type": "code",
        "outputId": "7afd1a6e-58e1-4292-ac6c-604f58d0ecd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "pip install -U keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uosl74kFL_7D",
        "colab_type": "code",
        "outputId": "5586c071-683d-489d-f3dc-c9f49cb23a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Freeze the layers\n",
        "for layer in vgg16_model.layers:\n",
        "    layer.trainable = False\n",
        "print(\"Pretrained layers are freezed\")\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretrained layers are freezed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD5xdmxmL_7J",
        "colab_type": "code",
        "outputId": "391b6e73-ffc6-42e4-87a5-07293fe9c3ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(vgg16_model)\n",
        "#add fully connected layer:\n",
        "input_layer=model.add(Flatten())\n",
        "hidden_layer=model.add(Dense(128, activation='relu', name='hidden_layer'))\n",
        "classification_layer=model.add(Dense(64, activation='relu', name='classification_layer'))\n",
        "output_layer=model.add(Dense(1, activation='sigmoid', name='output_layer')) \n",
        "print(\"All layers top of pretrained layers are developed\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All layers top of pretrained layers are developed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtMH9DEhL_7Q",
        "colab_type": "code",
        "outputId": "2a5a5ef1-7379-429e-8889-a9c4edacfb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_dir ='/content/drive/My Drive/Dataset1/training_set'\n",
        "val_data_dir ='/content/drive/My Drive/Dataset1/testing_set'\n",
        "model_weights_file = 'vgg16-xfer-weights.h5'\n",
        "nb_train_samples = 2117\n",
        "nb_val_samples = 563\n",
        "nb_epochs = 35\n",
        "print(\"Input parameters are assigned\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input parameters are assigned\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Olk9_b4L_7V",
        "colab_type": "code",
        "outputId": "83a4af67-bda0-4330-9e1e-4b1643a79805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Model is Complied\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is Complied\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noo6mamOTfZa",
        "colab_type": "code",
        "outputId": "8cd1e279-8a4d-412c-e3da-09a5d3d865ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz3f4zudL_7c",
        "colab_type": "code",
        "outputId": "0c9ce790-d990-4f7f-e17c-9aaf0d8f99b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_width, img_height), \n",
        "                                                    batch_size=32,shuffle=False, class_mode='binary')\n",
        "validation_generator = test_datagen.flow_from_directory(val_data_dir, target_size=(img_width, img_height), \n",
        "                                                    batch_size=32,shuffle=False,class_mode='binary')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2112 images belonging to 2 classes.\n",
            "Found 544 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1RBb4ExiBa5",
        "colab_type": "code",
        "outputId": "89901f97-dbc0-4a95-9c50-ea8ba42da64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "folder = '/content/drive/My Drive/Dataset1/training_set/'\n",
        "photos, labels = list(), list()\n",
        "for r, d, f in os.walk(folder):\n",
        "    for file in f:\n",
        "        if \".png\" in file:\n",
        "          if(os.path.join(r, file)=='/content/drive/My Drive/Dataset1/training_set/genuine/{}'.format(file)):\n",
        "            output=1\n",
        "          elif(os.path.join(r, file)=='/content/drive/My Drive/Dataset1/training_set/forged/{}'.format(file)):\n",
        "            output=0\n",
        "        labels.append(output)\n",
        "labels_train=np.array(labels)\n",
        "print(os.path.join(r, file))\n",
        "print(labels_train.shape)\n",
        "labels_train=np.expand_dims(labels, axis=1)\n",
        "print(labels_train)\n",
        "print(labels_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Dataset1/training_set/genuine/006_01.PNG\n",
            "(2117,)\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "(2117, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8UdZCwclWk9",
        "colab_type": "code",
        "outputId": "71d84b1e-cfb1-44f5-f9fc-82a4826f6c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "folder = '/content/drive/My Drive/Dataset1/testing_set/'\n",
        "photos, labels = list(), list()\n",
        "for r, d, f in os.walk(folder):\n",
        "    for file in f:\n",
        "        if \".png\" in file:\n",
        "          if(os.path.join(r, file)=='/content/drive/My Drive/Dataset1/testing_set/genuine/{}'.format(file)):\n",
        "            output=1\n",
        "          elif(os.path.join(r, file)=='/content/drive/My Drive/Dataset1/testing_set/forged/{}'.format(file)):\n",
        "            output=0\n",
        "        labels.append(output)\n",
        "labels_test=np.array(labels)\n",
        "print(os.path.join(r, file))\n",
        "print(labels_test.shape)\n",
        "labels_test=np.expand_dims(labels_test, axis=1)\n",
        "print(labels_test)\n",
        "print(labels_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Dataset1/testing_set/genuine/NFI-03001030.PNG\n",
            "(563,)\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "(563, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhasJCgnL_7g",
        "colab_type": "code",
        "outputId": "adc53493-fc2a-48bd-9c8d-66a1b3af9a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "callbacks = [ModelCheckpoint(model_weights_file, monitor='val_acc', save_best_only=True)]\n",
        "\n",
        "history = model.fit_generator( train_generator, callbacks = callbacks, nb_epoch=nb_epochs, validation_data=validation_generator)\n",
        "\n",
        "print('Training Completed!')\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "66/66 [==============================] - 1064s 16s/step - loss: 1.0251 - accuracy: 0.4413 - val_loss: 1.2816 - val_accuracy: 0.4926\n",
            "Epoch 2/35\n",
            "66/66 [==============================] - 17s 253ms/step - loss: 0.7338 - accuracy: 0.4744 - val_loss: 0.4723 - val_accuracy: 0.5074\n",
            "Epoch 3/35\n",
            "66/66 [==============================] - 18s 273ms/step - loss: 0.6968 - accuracy: 0.5459 - val_loss: 0.7270 - val_accuracy: 0.7574\n",
            "Epoch 4/35\n",
            "66/66 [==============================] - 18s 274ms/step - loss: 0.6797 - accuracy: 0.6274 - val_loss: 0.5544 - val_accuracy: 0.6397\n",
            "Epoch 5/35\n",
            "66/66 [==============================] - 18s 277ms/step - loss: 0.6635 - accuracy: 0.5800 - val_loss: 0.3053 - val_accuracy: 0.5074\n",
            "Epoch 6/35\n",
            "66/66 [==============================] - 19s 284ms/step - loss: 0.6505 - accuracy: 0.6501 - val_loss: 0.5895 - val_accuracy: 0.7408\n",
            "Epoch 7/35\n",
            "66/66 [==============================] - 18s 278ms/step - loss: 0.6660 - accuracy: 0.6259 - val_loss: 0.9011 - val_accuracy: 0.7169\n",
            "Epoch 8/35\n",
            "66/66 [==============================] - 18s 273ms/step - loss: 0.6293 - accuracy: 0.6359 - val_loss: 0.4331 - val_accuracy: 0.6801\n",
            "Epoch 9/35\n",
            "66/66 [==============================] - 18s 275ms/step - loss: 0.5403 - accuracy: 0.7197 - val_loss: 0.6268 - val_accuracy: 0.8107\n",
            "Epoch 10/35\n",
            "66/66 [==============================] - 18s 275ms/step - loss: 0.5859 - accuracy: 0.6828 - val_loss: 0.6426 - val_accuracy: 0.8217\n",
            "Epoch 11/35\n",
            "66/66 [==============================] - 18s 280ms/step - loss: 0.5011 - accuracy: 0.7599 - val_loss: 0.3140 - val_accuracy: 0.6912\n",
            "Epoch 12/35\n",
            "66/66 [==============================] - 18s 277ms/step - loss: 0.5257 - accuracy: 0.7420 - val_loss: 0.5392 - val_accuracy: 0.8290\n",
            "Epoch 13/35\n",
            "66/66 [==============================] - 19s 286ms/step - loss: 0.5034 - accuracy: 0.7543 - val_loss: 0.8425 - val_accuracy: 0.7923\n",
            "Epoch 14/35\n",
            "66/66 [==============================] - 18s 278ms/step - loss: 0.5321 - accuracy: 0.7363 - val_loss: 0.9660 - val_accuracy: 0.7537\n",
            "Epoch 15/35\n",
            "66/66 [==============================] - 19s 282ms/step - loss: 0.4802 - accuracy: 0.7765 - val_loss: 0.6401 - val_accuracy: 0.8235\n",
            "Epoch 16/35\n",
            "66/66 [==============================] - 18s 276ms/step - loss: 0.5553 - accuracy: 0.7330 - val_loss: 0.4290 - val_accuracy: 0.8033\n",
            "Epoch 17/35\n",
            "66/66 [==============================] - 18s 279ms/step - loss: 0.5053 - accuracy: 0.7670 - val_loss: 0.2205 - val_accuracy: 0.6820\n",
            "Epoch 18/35\n",
            "66/66 [==============================] - 18s 278ms/step - loss: 0.4500 - accuracy: 0.7879 - val_loss: 1.1699 - val_accuracy: 0.7721\n",
            "Epoch 19/35\n",
            "66/66 [==============================] - 18s 280ms/step - loss: 0.4337 - accuracy: 0.7945 - val_loss: 0.8838 - val_accuracy: 0.8327\n",
            "Epoch 20/35\n",
            "66/66 [==============================] - 18s 278ms/step - loss: 0.4226 - accuracy: 0.8134 - val_loss: 0.3141 - val_accuracy: 0.8364\n",
            "Epoch 21/35\n",
            "66/66 [==============================] - 18s 277ms/step - loss: 0.4209 - accuracy: 0.8035 - val_loss: 0.6690 - val_accuracy: 0.8456\n",
            "Epoch 22/35\n",
            "66/66 [==============================] - 18s 278ms/step - loss: 0.3650 - accuracy: 0.8456 - val_loss: 0.4251 - val_accuracy: 0.8750\n",
            "Epoch 23/35\n",
            "66/66 [==============================] - 19s 286ms/step - loss: 0.4315 - accuracy: 0.8078 - val_loss: 0.6717 - val_accuracy: 0.8493\n",
            "Epoch 24/35\n",
            "66/66 [==============================] - 18s 280ms/step - loss: 0.3781 - accuracy: 0.8385 - val_loss: 0.5796 - val_accuracy: 0.8713\n",
            "Epoch 25/35\n",
            "66/66 [==============================] - 18s 278ms/step - loss: 0.3473 - accuracy: 0.8518 - val_loss: 1.0136 - val_accuracy: 0.7941\n",
            "Epoch 26/35\n",
            "66/66 [==============================] - 18s 277ms/step - loss: 0.3589 - accuracy: 0.8456 - val_loss: 0.4453 - val_accuracy: 0.8768\n",
            "Epoch 27/35\n",
            "66/66 [==============================] - 18s 275ms/step - loss: 0.3213 - accuracy: 0.8627 - val_loss: 0.7292 - val_accuracy: 0.8456\n",
            "Epoch 28/35\n",
            "66/66 [==============================] - 18s 280ms/step - loss: 0.3170 - accuracy: 0.8688 - val_loss: 0.3852 - val_accuracy: 0.8934\n",
            "Epoch 29/35\n",
            "66/66 [==============================] - 18s 276ms/step - loss: 0.3473 - accuracy: 0.8466 - val_loss: 0.4361 - val_accuracy: 0.8860\n",
            "Epoch 30/35\n",
            "66/66 [==============================] - 18s 276ms/step - loss: 0.3204 - accuracy: 0.8594 - val_loss: 0.8778 - val_accuracy: 0.7776\n",
            "Epoch 31/35\n",
            "66/66 [==============================] - 18s 277ms/step - loss: 0.3326 - accuracy: 0.8461 - val_loss: 0.2238 - val_accuracy: 0.8768\n",
            "Epoch 32/35\n",
            "66/66 [==============================] - 18s 277ms/step - loss: 0.3384 - accuracy: 0.8490 - val_loss: 1.3105 - val_accuracy: 0.6728\n",
            "Epoch 33/35\n",
            "66/66 [==============================] - 19s 282ms/step - loss: 0.2898 - accuracy: 0.8759 - val_loss: 0.9229 - val_accuracy: 0.7996\n",
            "Epoch 34/35\n",
            "66/66 [==============================] - 19s 294ms/step - loss: 0.3282 - accuracy: 0.8480 - val_loss: 0.7376 - val_accuracy: 0.8493\n",
            "Epoch 35/35\n",
            "66/66 [==============================] - 19s 281ms/step - loss: 0.2701 - accuracy: 0.8854 - val_loss: 0.6947 - val_accuracy: 0.8438\n",
            "Training Completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrqW4KwQL_7r",
        "colab_type": "code",
        "outputId": "c733f848-e01e-40de-ddaa-41eda929d592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        " # save model and architecture to single file\n",
        "model.save(\"/content/drive/My Drive/Dataset1/training_set/model_vggnet_signature_verification.h5\")\n",
        "model.summary()\n",
        "\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer (Dense)         (None, 128)               1048704   \n",
            "_________________________________________________________________\n",
            "classification_layer (Dense) (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 15,771,713\n",
            "Trainable params: 1,057,025\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xgkyiPsnGms",
        "colab_type": "code",
        "outputId": "2aada7b6-2265-463c-d1d5-759312272c47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loading saved model from Drive.\n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/drive/My Drive/Dataset1/training_set/model_vggnet_signature_verification.h5\")\n",
        "print(\"Model is Loaded\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiBbsh5-19D6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMpmZaCJFPxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "layer_name= 'classification_layer'\n",
        "model = load_model(\"/content/drive/My Drive/Dataset1/training_set/model_vggnet_signature_verification.h5\")\n",
        "intermediate_layer_model = Model(inputs=model.input,\n",
        "                                 outputs=model.get_layer(layer_name).output)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ls3jvBMEkj_",
        "colab_type": "code",
        "outputId": "99632a8c-ad40-485c-ee43-8fc81d2979b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compilation of intermediate model\n",
        "intermediate_layer_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Model is Complied\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is Complied\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hivj_psZpbrU",
        "colab_type": "code",
        "outputId": "4947471a-6319-4ff4-be15-0dcd638381f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Saving intermediate model\n",
        "intermediate_layer_model.save(\"/content/drive/My Drive/Dataset1/training_set/intermediate_model_vggnet_signature_verification.h5\")\n",
        "intermediate_layer_model.summary()\n",
        "\n",
        "print(\"Saved Intermediate model to disk\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16_input (InputLayer)     (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer (Dense)         (None, 128)               1048704   \n",
            "_________________________________________________________________\n",
            "classification_layer (Dense) (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 15,771,648\n",
            "Trainable params: 1,056,960\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Saved Intermediate model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT373FILp0_e",
        "colab_type": "code",
        "outputId": "17a24890-32eb-434e-a5ec-d4569621497f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loading Intermediate Model\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/My Drive/Dataset1/training_set/intermediate_model_vggnet_signature_verification.h5\")\n",
        "print(\"Intermediate model is loaded\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intermediate model is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tawP0q3yqGBO",
        "colab_type": "code",
        "outputId": "71c2e269-ce19-4dd1-90ab-dd188b69cef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Label feature identification(y_train)\n",
        " import numpy as np\n",
        " batch_size=32\n",
        " sample_count=2112\n",
        " features = np.zeros(shape=(2112, 64))  # Must be equal to the output of the convolutional base\n",
        " labels = np.zeros(shape=(2112))\n",
        " i = 0\n",
        " for inputs_batch, labels_batch in train_generator:\n",
        "   features_batch = model.predict(inputs_batch)\n",
        "   features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
        "   labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
        "   i += 1\n",
        "   if i*batch_size  >= sample_count:\n",
        "     break\n",
        " print(labels.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2112,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl7T9UrMqKGo",
        "colab_type": "code",
        "outputId": "a6af1835-0cfa-45f6-bd48-6ab79ab0972a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#identification of training Labels\n",
        "features_train=features\n",
        "print(features_train.shape)\n",
        "labels_train=np.expand_dims(labels, axis=1)\n",
        "print(labels_train.shape)\n",
        "print(labels_train)#identification of training Labels\n",
        "features_train=features\n",
        "print(features_train.shape)\n",
        "print(features_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2112, 64)\n",
            "(2112, 1)\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "(2112, 64)\n",
            "[[0.         0.         0.         ... 0.         0.21225607 2.53358555]\n",
            " [0.         0.         0.         ... 0.         0.15670645 1.83832383]\n",
            " [0.         0.         0.         ... 0.         0.19858941 2.32724977]\n",
            " ...\n",
            " [0.         0.         0.268875   ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52524304 ... 0.         0.17369676 0.        ]\n",
            " [0.         0.         0.81019866 ... 0.4414255  0.677885   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdMzIPE1qNhX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f009b0a0-161a-4972-ee18-a8c00cee5001"
      },
      "source": [
        "#identification of training Labels\n",
        "features_train=features\n",
        "print(features_train.shape)\n",
        "labels_train=np.expand_dims(labels, axis=1)\n",
        "print(labels_train.shape)\n",
        "print(labels_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2112, 64)\n",
            "(2112, 1)\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SBF1J3vqQ4W",
        "colab_type": "code",
        "outputId": "32e1f17b-0bf5-4ee5-dd5c-5e1c5fe37168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# identification of test labels\n",
        " import numpy as np\n",
        " batch_size=32\n",
        " sample_count=544\n",
        " features_test = np.zeros(shape=(544, 64))  # Must be equal to the output of the convolutional base\n",
        " labels_test = np.zeros(shape=(544))\n",
        " i = 0\n",
        " for inputs_batch, labels_batch in validation_generator:\n",
        "   features_batch = model.predict(inputs_batch)\n",
        "   features_test[i * batch_size: (i + 1) * batch_size] = features_batch\n",
        "   labels_test[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
        "   i += 1\n",
        "   if i*batch_size  >= sample_count:\n",
        "     break\n",
        " print(labels_test.shape)\n",
        " print(features_test.shape)\n",
        " print(labels_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(544,)\n",
            "(544, 64)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dih8QbQqUel",
        "colab_type": "code",
        "outputId": "1c2a8e4c-4d6c-4054-b1b5-8b63dfcf5db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# #identification of testing Labels\n",
        "print(features_test.shape)\n",
        "print(features_test)\n",
        "labels_test=np.expand_dims(labels_test, axis=1)\n",
        "print(labels_test.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(544, 64)\n",
            "[[0.         0.         0.         ... 0.         0.         3.11525917]\n",
            " [0.         0.         0.         ... 0.         0.         2.32924747]\n",
            " [0.         0.         0.         ... 0.         0.         2.57253027]\n",
            " ...\n",
            " [0.         0.         0.1620345  ... 0.         0.4518519  0.        ]\n",
            " [0.         0.         0.         ... 0.         0.23946443 0.34716994]\n",
            " [0.         0.         0.4638266  ... 0.         0.51649761 0.        ]]\n",
            "(544, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYX4UkEYqZJb",
        "colab_type": "code",
        "outputId": "f6282e55-73f4-43d3-854e-de69d40303fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# SVM\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(features_train)\n",
        "X_test = sc.transform(features_test)\n",
        "\n",
        "# Fitting SVM to the Training set\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel = 'linear', random_state = 0)\n",
        "classifier.fit(X_train,labels_train.ravel())\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(labels_test, y_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(labels_test, y_pred))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[248  20]\n",
            " [ 37 239]]\n",
            "0.8952205882352942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piTDuGdvNd9z",
        "colab_type": "code",
        "outputId": "472d31ba-091e-4b06-981e-5752b27ba91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "# Kernel SVM\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(features_train)\n",
        "X_test = sc.transform(features_test)\n",
        "\n",
        "# Fitting KernelSVM to the Training set\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
        "classifier.fit(X_train, labels_train.ravel())\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(labels_test, y_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(labels_test, y_pred))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[249  19]\n",
            " [ 40 236]]\n",
            "0.8915441176470589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeyaANSC1zeA",
        "colab_type": "code",
        "outputId": "038d2f13-f753-4208-984a-b12b4010b1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Random Forest\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(features_train)\n",
        "X_test = sc.transform(features_test)\n",
        "\n",
        "# Fitting KernelSVM to the Training set\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, labels_train.ravel())\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(labels_test, y_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(labels_test, y_pred))\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[250  18]\n",
            " [ 35 241]]\n",
            "0.9025735294117647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qBoJeVA7cA0",
        "colab_type": "code",
        "outputId": "67cf7201-d0b8-4591-f619-93779eb4e259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Decision Tree\n",
        "# Feature Scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(features_train)\n",
        "X_test = sc.transform(features_test)\n",
        "\n",
        "# Fitting KernelSVM to the Training set\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, labels_train.ravel())\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(labels_test, y_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(labels_test, y_pred))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[250  18]\n",
            " [ 50 226]]\n",
            "0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUxX29IQ8Pex",
        "colab_type": "code",
        "outputId": "80a96011-073e-4571-cc48-f11fbdf4b701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "# Naive Bayes\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(features_train)\n",
        "X_test = sc.transform(features_test)\n",
        "\n",
        "# Fitting KernelSVM to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, labels_train.ravel())\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(labels_test, y_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(labels_test, y_pred))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[251  17]\n",
            " [ 48 228]]\n",
            "0.8805147058823529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrmXyTdm8nUR",
        "colab_type": "code",
        "outputId": "f2a61dcc-391f-46d8-d11b-019b9d22f020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(features_train)\n",
        "X_test = sc.transform(features_test)\n",
        "\n",
        "# Fitting KernelSVM to the Training set\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier =  KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "classifier.fit(X_train, labels_train.ravel())\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(labels_test, y_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(labels_test, y_pred))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[249  19]\n",
            " [ 32 244]]\n",
            "0.90625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4qIzREF8-uz",
        "colab_type": "code",
        "outputId": "aacd63d3-4562-4835-d9bb-60dde7a4eba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state = 0)\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(features_train)\n",
        "X_test = sc.transform(features_test)\n",
        "\n",
        "# Fitting KernelSVM to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state = 0)\n",
        "classifier.fit(X_train, labels_train.ravel())\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(labels_test, y_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(labels_test, y_pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[249  19]\n",
            " [ 39 237]]\n",
            "0.8933823529411765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8a28e26a-bde2-4e1b-eed0-f5983140b84a",
        "id": "sCESN-7A9ejM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/My Drive/Dataset1/training_set/model_vggnet_signature_verification.h5\")\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print('\\n# Evaluate on test data')\n",
        "results = model.evaluate(validation_generator)\n",
        "print('test loss, test acc:', results)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "17/17 [==============================] - 2s 145ms/step\n",
            "test loss, test acc: [0.69471275806427, 0.84375]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}